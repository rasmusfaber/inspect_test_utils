import os
import random
import tempfile
import textwrap
from typing import Any, Literal

import yaml
from inspect_ai import task, Task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes, Score
from inspect_ai.solver import solver, TaskState, Generate, use_tools, generate
from inspect_ai.tool import bash, python

from inspect_test_utils import scorers


@solver
def failing_solver(
        fail_on_epochs: list[int] | None = None,
        failure_rate: float = 0.2,
):
    async def solve(state: TaskState, generate: Generate):
        if fail_on_epochs is None or state.epoch in fail_on_epochs:
            if random.random() < failure_rate:
                raise ValueError("Eval failed!")

        return state

    return solve


@task
def sometimes_fails_setup(
        sample_count: int = 10,
        fail_setup_on_epochs: list[int] | None = None,
        failure_rate: float = 0.2,
) -> Task:
    return Task(
        dataset=[
            Sample(id=str(i), input="Say hello", target="hello") for i in range(sample_count)
        ],
        setup=failing_solver(fail_on_epochs=fail_setup_on_epochs, failure_rate=failure_rate),
        scorer=includes(),
        sandbox="docker",
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )


@task
def sometimes_fails_scoring(
        sample_count: int = 10,
        fail_score_on_epochs: list[int] | None = None,
        failure_rate: float = 0.2,
) -> Task:
    return Task(
        dataset=[
            Sample(id=str(i), input="Say hello", target="hello") for i in range(sample_count)
        ],
        scorer=scorers.failing_scorer(fail_on_epochs=fail_score_on_epochs, failure_rate=failure_rate),
        sandbox="docker",
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )


@task
def hardcoded_score(
        sample_count: int = 10,
        hardcoded_score: Score | None = None,
        hardcoded_score_by_sample_id_and_epoch: dict[str, dict[int, dict[str, Any]]] | None = None,
) -> Task:
    return Task(
        dataset=[
            Sample(id=str(i), input="Say hello", target="hello") for i in range(sample_count)
        ],
        scorer=scorers.hardcoded_scorer(hardcoded_score, hardcoded_score_by_sample_id_and_epoch),
        sandbox="docker",
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )


@task
def say_hello(
        sample_count: int = 1,
) -> Task:
    return Task(
        dataset=[
            Sample(id=str(i), input="Say hello", target="hello") for i in range(sample_count)
        ],
        scorer=includes(),
        sandbox="docker",
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )


@task
def guess_number(
        sample_count: int = 1,
        target: str = "42.7",
) -> Task:
    return Task(
        dataset=[
            Sample(id=str(i), input="Guess the number", target=target) for i in range(sample_count)
        ],
        scorer=scorers.closeness_log(),
        sandbox="docker",
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )


@task
def configurable_sandbox(
        sample_count: int = 1,
        cpu: float = 0.5,
        memory: str = "2G",
        storage: str = "2G",
        gpu: int | None = None,
        gpu_model: Literal["t4", "h100"] | None = None,
        allow_internet: bool = False,
) -> Task:
    # Write a compose.yaml to a temporary file:
    tmpdir = tempfile.mkdtemp(prefix="inspect_test_utils_")
    values_yaml_path = os.path.join(tmpdir, "values.yaml")
    values: dict[str, Any] = {
        "services": {
            "default": {
                "image": "python:3.12-bookworm",
                "args": ["tail", "-f", "/dev/null"],
                "resources": {
                    "requests": {
                        "cpu": cpu,
                        "memory": memory,
                        "ephemeral-storage": storage,
                    },
                    "limits": {
                        "cpu": cpu,
                        "memory": memory,
                        "ephemeral-storage": storage,
                    }
                }
            }
        }
    }
    if gpu is not None:
        values["services"]["default"]["image"] = "nvidia/cuda:12.4.1-devel-ubuntu22.04"
        values["services"]["default"]["runtimeClassName"] = "nvidia"
        values["services"]["default"]["resources"]["requests"]["nvidia.com/gpu"] = gpu
        values["services"]["default"]["resources"]["limits"]["nvidia.com/gpu"] = gpu
        values["services"]["default"]["env"] = [
            {"name": "NVIDIA_DRIVER_CAPABILITIES", "value": "compute,utility"}
        ]
        if gpu_model == "t4":
            values["services"]["default"]["nodeSelector"] = {
                "karpenter.k8s.aws/instance-gpu-name": "t4"
            }
        elif gpu_model == "h100":
            values["services"]["default"]["nodeSelector"] = {
                "nvidia.com/gpu.product": "NVIDIA-H100-80GB-HBM3"
            }
    if allow_internet:
        values["allowEntities"] = ["world"]
    values_yaml = yaml.dump(values)
    with open(values_yaml_path, "w", encoding="utf-8") as f:
        f.write(values_yaml)

    return Task(
        dataset=[
            Sample(id=str(i), input="Say hello", target="hello") for i in range(sample_count)
        ],
        scorer=includes(),
        sandbox=("k8s", values_yaml_path),
        solver=[
            use_tools(bash(), python()),
            generate(),
        ]
    )
